version: "3.7"

services:
  # This service runs the gRPC server that loads your user code, in both dagit
  # and dagster-daemon. By setting DAGSTER_CURRENT_IMAGE to its own image, we tell the
  # run launcher to use this same image when launching runs in a new container as well.
  # Multiple containers like this can be deployed separately - each just needs to run on
  # its own port, and have its own entry in the workspace.yaml file that's loaded by dagit.
  data_pipelines:
    build:
      context: .
      dockerfile: ./Dockerfile_data_pipelines
    container_name: data_pipelines
    image: danieldaum/data_pipelines:latest
    restart: always
    environment:
      DAGSTER_CURRENT_IMAGE: "danieldaum/data_pipelines:latest"
      TARGET: ${TARGET}
      WAREHOUSE: ${WAREHOUSE}
      AWS_ACCESS_KEY_ID: ${AWS_ACCESS_KEY_ID}
      AWS_SECRET_ACCESS_KEY: ${AWS_SECRET_ACCESS_KEY}
      DBT_ENV_SECRET_HOST: ${DBT_ENV_SECRET_HOST}
      DBT_ENV_SECRET_USER: ${DBT_ENV_SECRET_USER}
      DBT_ENV_SECRET_PASS: ${DBT_ENV_SECRET_PASS}
      DAGSTER_METADATA_DB: ${DAGSTER_METADATA_DB}
    networks:
      - dagster_network

  # This service runs dagit, which loads your user code from the user code container.
  # Since our instance uses the QueuedRunCoordinator, any runs submitted from dagit will be put on
  # a queue and later dequeued and launched by dagster-daemon.
  dagit:
    build:
      context: .
      dockerfile: ./Dockerfile_dagster
    entrypoint:
      - dagit
      - -h
      - "0.0.0.0"
      - -p
      - "3000"
      - -w
      - workspace.yaml
    container_name: dagit
    expose:
      - "3000"
    ports:
      - "3000:3000"
    image: danieldaum/dagit:latest
    environment:
      TARGET: ${TARGET}
      WAREHOUSE: ${WAREHOUSE}
      AWS_ACCESS_KEY_ID: ${AWS_ACCESS_KEY_ID}
      AWS_SECRET_ACCESS_KEY: ${AWS_SECRET_ACCESS_KEY}
      DBT_ENV_SECRET_HOST: ${DBT_ENV_SECRET_HOST}
      DBT_ENV_SECRET_USER: ${DBT_ENV_SECRET_USER}
      DBT_ENV_SECRET_PASS: ${DBT_ENV_SECRET_PASS}
      DAGSTER_METADATA_DB: ${DAGSTER_METADATA_DB}
    volumes:
      # Make docker client accessible so we can terminate containers from dagit
      - /var/run/docker.sock:/var/run/docker.sock
      - /tmp/io_manager_storage:/tmp/io_manager_storage
    networks:
      - dagster_network
    depends_on:
      - data_pipelines

  # This service runs the dagster-daemon process, which is responsible for taking runs
  # off of the queue and launching them, as well as creating runs from schedules or sensors.
  daemon:
    build:
      context: .
      dockerfile: ./Dockerfile_dagster
    entrypoint:
      - dagster-daemon
      - run
    container_name: daemon
    restart: on-failure
    image: danieldaum/daemon:latest
    environment:
      TARGET: ${TARGET}
      WAREHOUSE: ${WAREHOUSE}
      AWS_ACCESS_KEY_ID: ${AWS_ACCESS_KEY_ID}
      AWS_SECRET_ACCESS_KEY: ${AWS_SECRET_ACCESS_KEY}
      DBT_ENV_SECRET_HOST: ${DBT_ENV_SECRET_HOST}
      DBT_ENV_SECRET_USER: ${DBT_ENV_SECRET_USER}
      DBT_ENV_SECRET_PASS: ${DBT_ENV_SECRET_PASS}
      DAGSTER_METADATA_DB: ${DAGSTER_METADATA_DB}

    volumes:
      # Make docker client accessible so we can launch containers using host docker
      - /var/run/docker.sock:/var/run/docker.sock
      - /tmp/io_manager_storage:/tmp/io_manager_storage
    networks:
      - dagster_network
    depends_on:
      - data_pipelines

  metabase:
    image: metabase/metabase
    container_name: metabase
    environment:
      MB_DB_TYPE: postgres
      MB_DB_DBNAME: ${METABASE_METADATA_DB}
      MB_DB_PORT: 25060
      MB_DB_USER: ${DBT_ENV_SECRET_USER}
      MB_DB_PASS: ${DBT_ENV_SECRET_PASS}
      MB_DB_HOST: ${DBT_ENV_SECRET_HOST}
    ports:
      - 5000:3000
    restart: always
    volumes:
      # declare your mount volume /host/dir:/container/dir
      - /home/app/metabase-data:/metabase-data
    networks:
      - dagster_network

networks:
  dagster_network:
    driver: bridge
    name: dagster_network
